[
   {
      "author_association" : "MEMBER",
      "body" : "TODO:\r\n\r\n* [ ] Should be run on `make check`",
      "created_at" : "2018-03-29T15:47:44Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377279610",
      "id" : 377279610,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T15:47:50Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377279610",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "Concept ACK\r\n\r\nVery nice!",
      "created_at" : "2018-03-29T17:11:19Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377305854",
      "id" : 377305854,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T17:11:19Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377305854",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "Perhaps a stupid question but why the need of a static `test_list.txt`? What would be the disadvantages of generating the test list at run-time?",
      "created_at" : "2018-03-29T17:13:03Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377306416",
      "id" : 377306416,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T17:13:36Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377306416",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "`--list_content` is only documented in 1.60.0, so I assume it wouldn't work in previous versions. Currently we support 1.47.0+\r\n\r\nhttp://www.boost.org/doc/libs/1_60_0/libs/test/doc/html/boost_test/utf_reference/rt_param_reference/list_content.html\r\nhttps://github.com/bitcoin/bitcoin/blob/master/doc/dependencies.md#dependencies",
      "created_at" : "2018-03-29T17:28:05Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377311124",
      "id" : 377311124,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T17:28:05Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377311124",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@MarcoFalke The output from `git grep -E '(BOOST_FIXTURE_TEST_SUITE|BOOST_AUTO_TEST_CASE)' -- \"src/**.cpp\"` could perhaps be used to quickly (~14 ms on my machine) generate the list of available tests?",
      "created_at" : "2018-03-29T20:30:31Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377362816",
      "id" : 377362816,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T20:32:15Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377362816",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "If we go with the static text file `test_list.txt` I suggest adding a script `contrib/devtools/lint-test_list.sh` which checks that the list of tests in `test_list.txt` is in sync with the list of tests given by `src/test/test_bitcoin --list_content`.\r\n\r\nThat way Travis would automatically catch the case where someone adds a test and forgets to manually run `src/test/parallel.py --write_list` (which requires running Boost.Test 1.60.0 or above) and commit the changes made to `test_list.txt` after adding a new test.",
      "created_at" : "2018-03-29T20:39:18Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377365128",
      "id" : 377365128,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-29T20:41:09Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377365128",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178233629"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178233629"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "Should be `python3`Ã¯Â¿Â¼?",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T05:08:48Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178233629",
      "id" : 178233629,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 1,
      "path" : "src/test/parallel.py",
      "position" : 1,
      "pull_request_review_id" : 108264341,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T05:08:49Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178233629",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178234706"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178234706"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "This seems to be fine in python 2.7 but is a problem in 3.x\r\n```\r\nValueError: can't have unbuffered text I/O\r\n```\r\nThe 0 buffer is only valid for byte streams in python 3, from docs the default buffer policy for interactive text files is line buffering, which is probably fine. ",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T05:24:29Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import configparser\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178234706",
      "id" : 178234706,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 310,
      "path" : "src/test/parallel.py",
      "position" : 310,
      "pull_request_review_id" : 108265562,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T05:24:29Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178234706",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178235787"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178235787"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "not sure why abc/ is appended to path. Get\r\n```\r\nTest setup error: no test cases matching filter\r\n```\r\nRemoving abc/ lets tests run and pass. ",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T05:41:18Z",
      "diff_hunk" : "@@ -0,0 +1,285 @@\n+arith_uint256_tests/basics\n+arith_uint256_tests/shifts\n+arith_uint256_tests/unaryOperators\n+arith_uint256_tests/bitwiseOperators\n+arith_uint256_tests/comparison\n+arith_uint256_tests/plusMinus\n+arith_uint256_tests/multiply\n+arith_uint256_tests/divide\n+arith_uint256_tests/methods\n+arith_uint256_tests/bignum_SetCompact\n+arith_uint256_tests/getmaxcoverage\n+addrman_tests/addrman_simple\n+addrman_tests/addrman_ports\n+addrman_tests/addrman_select\n+addrman_tests/addrman_new_collisions\n+addrman_tests/addrman_tried_collisions\n+addrman_tests/addrman_find\n+addrman_tests/addrman_create\n+addrman_tests/addrman_delete\n+addrman_tests/addrman_getaddr\n+addrman_tests/caddrinfo_get_tried_bucket\n+addrman_tests/caddrinfo_get_new_bucket\n+addrman_tests/addrman_selecttriedcollision\n+addrman_tests/addrman_noevict\n+addrman_tests/addrman_evictionworks\n+amount_tests/MoneyRangeTest\n+amount_tests/GetFeeTest\n+amount_tests/BinaryOperatorTest\n+amount_tests/ToStringTest\n+allocator_tests/arena_tests\n+allocator_tests/lockedpool_tests_mock\n+allocator_tests/lockedpool_tests_live\n+base32_tests/base32_testvectors\n+base58_tests/base58_EncodeBase58\n+base58_tests/base58_DecodeBase58\n+base64_tests/base64_testvectors\n+abc/bech32_tests/bip173_testvectors_valid",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178235787",
      "id" : 178235787,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 37,
      "path" : "src/test/test_list.txt",
      "position" : 37,
      "pull_request_review_id" : 108266847,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T05:41:18Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178235787",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178236227"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178236227"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "This also poses an interesting problem. If the test case isn't found, its listed as `FAILED TESTS`, which is somewhat confusing, because `bech32_tests/bip173_testvectors_valid` would pass, it's just the path listed is wrong. \r\n\r\nMaybe the initial reader of `FILE_TEST_LIST` can verify the path exists before handing it off to workers. ",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T05:47:40Z",
      "diff_hunk" : "@@ -0,0 +1,285 @@\n+arith_uint256_tests/basics\n+arith_uint256_tests/shifts\n+arith_uint256_tests/unaryOperators\n+arith_uint256_tests/bitwiseOperators\n+arith_uint256_tests/comparison\n+arith_uint256_tests/plusMinus\n+arith_uint256_tests/multiply\n+arith_uint256_tests/divide\n+arith_uint256_tests/methods\n+arith_uint256_tests/bignum_SetCompact\n+arith_uint256_tests/getmaxcoverage\n+addrman_tests/addrman_simple\n+addrman_tests/addrman_ports\n+addrman_tests/addrman_select\n+addrman_tests/addrman_new_collisions\n+addrman_tests/addrman_tried_collisions\n+addrman_tests/addrman_find\n+addrman_tests/addrman_create\n+addrman_tests/addrman_delete\n+addrman_tests/addrman_getaddr\n+addrman_tests/caddrinfo_get_tried_bucket\n+addrman_tests/caddrinfo_get_new_bucket\n+addrman_tests/addrman_selecttriedcollision\n+addrman_tests/addrman_noevict\n+addrman_tests/addrman_evictionworks\n+amount_tests/MoneyRangeTest\n+amount_tests/GetFeeTest\n+amount_tests/BinaryOperatorTest\n+amount_tests/ToStringTest\n+allocator_tests/arena_tests\n+allocator_tests/lockedpool_tests_mock\n+allocator_tests/lockedpool_tests_live\n+base32_tests/base32_testvectors\n+base58_tests/base58_EncodeBase58\n+base58_tests/base58_DecodeBase58\n+base64_tests/base64_testvectors\n+abc/bech32_tests/bip173_testvectors_valid",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178236227",
      "id" : 178236227,
      "in_reply_to_id" : 178235787,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 37,
      "path" : "src/test/test_list.txt",
      "position" : 37,
      "pull_request_review_id" : 108267366,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T05:47:40Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178236227",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178236641"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178236641"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "Nit: if you are going to verify the input is valid, you can check some other fields as well\r\n\r\nRight now you could input a negative for `workers`, `repeat`, `timeout`, etc. without complaint",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T05:52:47Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import configparser\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n+\n+    self.output_dir = output_dir\n+\n+    self.total_tasks = 0\n+    self.finished_tasks = 0\n+    self.out = Outputter(sys.stdout)\n+    self.stdout_lock = threading.Lock()\n+\n+  def move_to(self, destination_dir, tasks):\n+    if self.output_dir is None:\n+      return\n+\n+    destination_dir = os.path.join(self.output_dir, destination_dir)\n+    os.makedirs(destination_dir)\n+    for task in tasks:\n+      shutil.move(task.log_file, destination_dir)\n+\n+  def print_tests(self, message, tasks, print_try_number):\n+    self.out.permanent_line(\"%s (%s/%s):\" %\n+                            (message, len(tasks), self.total_tasks))\n+    for task in sorted(tasks):\n+      runtime_ms = 'Interrupted'\n+      if task.runtime_ms is not None:\n+        runtime_ms = '%d ms' % task.runtime_ms\n+      self.out.permanent_line(\"%11s: %s %s%s\" % (\n+          runtime_ms, task.test_binary, task.test_name,\n+          (\" (try #%d)\" % task.execution_number) if print_try_number else \"\"))\n+\n+  def log_exit(self, task):\n+    with self.stdout_lock:\n+      self.finished_tasks += 1\n+      self.out.transient_line(\"[%d/%d] %s (%d ms)\"\n+                              % (self.finished_tasks, self.total_tasks,\n+                                 task.test_name, task.runtime_ms))\n+      if task.exit_code != 0:\n+        with open(task.log_file) as f:\n+          for line in f.readlines():\n+            self.out.permanent_line(line.rstrip())\n+        self.out.permanent_line(\n+          \"[%d/%d] %s returned/aborted with exit code %d (%d ms)\"\n+          % (self.finished_tasks, self.total_tasks, task.test_name,\n+             task.exit_code, task.runtime_ms))\n+\n+    if self.output_dir is None:\n+      # Try to remove the file 100 times (sleeping for 0.1 second in between).\n+      # This is a workaround for a process handle seemingly holding on to the\n+      # file for too long inside os.subprocess. This workaround is in place\n+      # until we figure out a minimal repro to report upstream (or a better\n+      # suspect) to prevent os.remove exceptions.\n+      num_tries = 100\n+      for i in range(num_tries):\n+        try:\n+          os.remove(task.log_file)\n+        except OSError as e:\n+          if e.errno is not errno.ENOENT:\n+            if i is num_tries - 1:\n+              self.out.permanent_line('Could not remove temporary log file: ' + str(e))\n+            else:\n+              time.sleep(0.1)\n+            continue\n+        break\n+\n+  def log_tasks(self, total_tasks):\n+    self.total_tasks += total_tasks\n+    self.out.transient_line(\"[0/%d] Running tests...\" % self.total_tasks)\n+\n+  def summarize(self, passed_tasks, failed_tasks, interrupted_tasks):\n+    stats = {}\n+    def add_stats(stats, task, idx):\n+      task_key = (task.test_binary, task.test_name)\n+      if not task_key in stats:\n+        # (passed, failed, interrupted) task_key is added as tie breaker to get\n+        # alphabetic sorting on equally-stable tests\n+        stats[task_key] = [0, 0, 0, task_key]\n+      stats[task_key][idx] += 1\n+\n+    for task in passed_tasks:\n+      add_stats(stats, task, 0)\n+    for task in failed_tasks:\n+      add_stats(stats, task, 1)\n+    for task in interrupted_tasks:\n+      add_stats(stats, task, 2)\n+\n+    self.out.permanent_line(\"SUMMARY:\")\n+    for task_key in sorted(stats, key=stats.__getitem__):\n+      (num_passed, num_failed, num_interrupted, _) = stats[task_key]\n+      (test_binary, task_name) = task_key\n+      self.out.permanent_line(\n+          \"  %s %s passed %d / %d times%s.\" %\n+              (test_binary, task_name, num_passed,\n+               num_passed + num_failed + num_interrupted,\n+               \"\" if num_interrupted == 0 else (\" (%d interrupted)\" % num_interrupted)))\n+\n+  def flush(self):\n+    self.out.flush_transient_output()\n+\n+\n+class CollectTestResults(object):\n+  def __init__(self, json_dump_filepath):\n+    self.test_results_lock = threading.Lock()\n+    self.json_dump_file = open(json_dump_filepath, 'w')\n+    self.test_results = {\n+        \"interrupted\": False,\n+        \"path_delimiter\": \".\",\n+        # Third version of the file format. See the link in the flag description\n+        # for details.\n+        \"version\": 3,\n+        \"seconds_since_epoch\": int(time.time()),\n+        \"num_failures_by_type\": {\n+            \"PASS\": 0,\n+            \"FAIL\": 0,\n+        },\n+        \"tests\": {},\n+    }\n+\n+  def log(self, test, runtime_ms, actual_result):\n+    with self.test_results_lock:\n+      self.test_results['num_failures_by_type'][actual_result] += 1\n+      results = self.test_results['tests']\n+      for name in test.split('.'):\n+        results = results.setdefault(name, {})\n+\n+      if results:\n+        results['actual'] += ' ' + actual_result\n+        results['times'].append(runtime_ms)\n+      else:  # This is the first invocation of the test\n+        results['actual'] = actual_result\n+        results['times'] = [runtime_ms]\n+        results['time'] = runtime_ms\n+        results['expected'] = 'PASS'\n+\n+  def dump_to_file_and_close(self):\n+    json.dump(self.test_results, self.json_dump_file)\n+    self.json_dump_file.close()\n+\n+\n+# Record of test runtimes. Has built-in locking.\n+class TestTimes(object):\n+  class LockedFile(object):\n+    def __init__(self, filename, mode):\n+      self._filename = filename\n+      self._mode = mode\n+      self._fo = None\n+\n+    def __enter__(self):\n+      self._fo = open(self._filename, self._mode)\n+\n+      # Regardless of opening mode we always seek to the beginning of file.\n+      # This simplifies code working with LockedFile and also ensures that\n+      # we lock (and unlock below) always the same region in file on win32.\n+      self._fo.seek(0)\n+\n+      try:\n+        if sys.platform == 'win32':\n+          # We are locking here fixed location in file to use it as\n+          # an exclusive lock on entire file.\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_LOCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_EX)\n+      except IOError:\n+        self._fo.close()\n+        raise\n+\n+      return self._fo\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+      # Flush any buffered data to disk. This is needed to prevent race\n+      # condition which happens from the moment of releasing file lock\n+      # till closing the file.\n+      self._fo.flush()\n+\n+      try:\n+        if sys.platform == 'win32':\n+          self._fo.seek(0)\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_UNLCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_UN)\n+      finally:\n+        self._fo.close()\n+\n+      return exc_value is None\n+\n+  def __init__(self, save_file):\n+    \"Create new object seeded with saved test times from the given file.\"\n+    self.__times = {}  # (test binary, test name) -> runtime in ms\n+\n+    # Protects calls to record_test_time(); other calls are not\n+    # expected to be made concurrently.\n+    self.__lock = threading.Lock()\n+\n+    try:\n+      with TestTimes.LockedFile(save_file, 'rb') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+    except IOError:\n+      # We couldn't obtain the lock.\n+      return\n+\n+    # Discard saved times if the format isn't right.\n+    if type(times) is not dict:\n+      return\n+    for ((test_binary, test_name), runtime) in list(times.items()):\n+      if (type(test_binary) is not str or type(test_name) is not str\n+          or type(runtime) not in {int, long, type(None)}):\n+        return\n+\n+    self.__times = times\n+\n+  def get_test_time(self, binary, testname):\n+    \"\"\"Return the last duration for the given test as an integer number of\n+    milliseconds, or None if the test failed or if there's no record for it.\"\"\"\n+    return self.__times.get((binary, testname), None)\n+\n+  def record_test_time(self, binary, testname, runtime_ms):\n+    \"\"\"Record that the given test ran in the specified number of\n+    milliseconds. If the test failed, runtime_ms should be None.\"\"\"\n+    with self.__lock:\n+      self.__times[(binary, testname)] = runtime_ms\n+\n+  def write_to_file(self, save_file):\n+    \"Write all the times to file.\"\n+    try:\n+      with TestTimes.LockedFile(save_file, 'a+b') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+\n+        if times is None:\n+          times = self.__times\n+        else:\n+          times.update(self.__times)\n+\n+        # We erase data from file while still holding a lock to it. This\n+        # way reading old test times and appending new ones are atomic\n+        # for external viewer.\n+        fd.seek(0)\n+        fd.truncate()\n+        with gzip.GzipFile(fileobj=fd, mode='wb') as gzf:\n+          cPickle.dump(times, gzf, PICKLE_HIGHEST_PROTOCOL)\n+    except IOError:\n+      pass  # ignore errors---saving the times isn't that important\n+\n+  @staticmethod\n+  def __read_test_times_file(fd):\n+    try:\n+      with gzip.GzipFile(fileobj=fd, mode='rb') as gzf:\n+        times = cPickle.load(gzf)\n+    except Exception:\n+      # File doesn't exist, isn't readable, is malformed---whatever.\n+      # Just ignore it.\n+      return None\n+    else:\n+      return times\n+\n+\n+def find_tests(test_list, binaries, additional_args, options, times):\n+  test_count = 0\n+  tasks = []\n+  for test_binary in binaries:\n+    command = [test_binary]\n+    command += additional_args\n+\n+    for test_name in test_list:\n+      last_execution_time = times.get_test_time(test_binary, test_name)\n+      if options.failed and last_execution_time is not None:\n+        continue\n+\n+      test_command = command + ['--run_test=' + test_name]\n+      if (test_count - options.shard_index) % options.shard_count == 0:\n+        for execution_number in range(options.repeat):\n+          tasks.append(Task(test_binary, test_name, test_command,\n+                            execution_number + 1, last_execution_time,\n+                            options.output_dir))\n+\n+      test_count += 1\n+\n+  # Sort the tasks to run the slowest tests first, so that faster ones can be\n+  # finished in parallel.\n+  return sorted(tasks, reverse=True)\n+\n+\n+def execute_tasks(tasks, pool_size, task_manager,\n+                  timeout, serialize_test_cases):\n+  class WorkerFn(object):\n+    def __init__(self, tasks, running_groups):\n+      self.tasks = tasks\n+      self.running_groups = running_groups\n+      self.task_lock = threading.Lock()\n+\n+    def __call__(self):\n+      while True:\n+        with self.task_lock:\n+          for task_id in range(len(self.tasks)):\n+            task = self.tasks[task_id]\n+\n+            if self.running_groups is not None:\n+              test_group = task.test_name.split('.')[0]\n+              if test_group in self.running_groups:\n+                # Try to find other non-running test group.\n+                continue\n+              else:\n+                self.running_groups.add(test_group)\n+\n+            del self.tasks[task_id]\n+            break\n+          else:\n+            # Either there is no tasks left or number or remaining test\n+            # cases (groups) is less than number or running threads.\n+            return\n+\n+        task_manager.run_task(task)\n+\n+        if self.running_groups is not None:\n+          with self.task_lock:\n+            self.running_groups.remove(test_group)\n+\n+  def start_daemon(func):\n+    t = threading.Thread(target=func)\n+    t.daemon = True\n+    t.start()\n+    return t\n+\n+  try:\n+    if timeout:\n+      timeout.start()\n+    running_groups = set() if serialize_test_cases else None\n+    worker_fn = WorkerFn(tasks, running_groups)\n+    workers = [start_daemon(worker_fn) for _ in range(pool_size)]\n+    for worker in workers:\n+      worker.join()\n+  finally:\n+    if timeout:\n+      timeout.cancel()\n+\n+\n+def default_options_parser(default_binary, file_test_list):\n+  parser = optparse.OptionParser(\n+      usage = 'usage: %prog [options] binary [binary ...] -- [additional args]')\n+\n+  parser.add_option('-d', '--output_dir', type='string', default=None,\n+                    help='Output directory for test logs. Logs will be '\n+                         'available under gtest-parallel-logs/, so '\n+                         '--output_dir=/tmp will results in all logs being '\n+                         'available under /tmp/gtest-parallel-logs/.')\n+  parser.add_option('-r', '--repeat', type='int', default=1,\n+                    help='Number of times to execute all the tests.')\n+  parser.add_option('--retry_failed', type='int', default=0,\n+                    help='Number of times to repeat failed tests.')\n+  parser.add_option('--failed', action='store_true', default=False,\n+                    help='run only failed and new tests')\n+  parser.add_option('-w', '--workers', type='int',\n+                    default=multiprocessing.cpu_count(),\n+                    help='number of workers to spawn')\n+  parser.add_option('--gtest_color', type='string', default='yes',\n+                    help='color output')\n+  parser.add_option('--gtest_filter', type='string', default='',\n+                    help='test filter')\n+  parser.add_option('--gtest_also_run_disabled_tests', action='store_true',\n+                    default=False, help='run disabled tests too')\n+  parser.add_option('--print_test_times', action='store_true', default=False,\n+                    help='list the run time of each test at the end of execution')\n+  parser.add_option('--shard_count', type='int', default=1,\n+                    help='total number of shards (for sharding test execution '\n+                         'between multiple machines)')\n+  parser.add_option('--shard_index', type='int', default=0,\n+                    help='zero-indexed number identifying this shard (for '\n+                         'sharding test execution between multiple machines)')\n+  parser.add_option('--dump_json_test_results', type='string', default=None,\n+                    help='Saves the results of the tests as a JSON machine-'\n+                         'readable file. The format of the file is specified at '\n+                         'https://www.chromium.org/developers/the-json-test-results-format')\n+  parser.add_option('--timeout', type='int', default=None,\n+                    help='Interrupt all remaining processes after the given '\n+                         'time (in seconds).')\n+  parser.add_option('--serialize_test_cases', action='store_true',\n+                    default=False, help='Do not run tests from the same test '\n+                                        'case in parallel.')\n+# Bitcoin Core specific options parsing: start\n+  parser.add_option('-l', '--write_list', action='store_true',\n+                    default=False, help='Write test list from the binary to '\n+                                        '{} and exit.'.format(file_test_list))\n+  parser.add_option('-b', '--binary', type=str, default=default_binary,\n+                    help='The test binary (default: {})'.format(default_binary))\n+# Bitcoin Core specific options parsing: end\n+  return parser\n+\n+\n+def main():\n+  # Remove additional arguments (anything after --).\n+  additional_args = []\n+\n+  for i in range(len(sys.argv)):\n+    if sys.argv[i] == '--':\n+      additional_args = sys.argv[i+1:]\n+      sys.argv = sys.argv[:i]\n+      break\n+# Bitcoin Core specific options parsing: start\n+  config = configparser.ConfigParser()\n+  configfile = os.path.abspath(os.path.dirname(__file__)) + \"/../../test/config.ini\"\n+  config.read_file(open(configfile))\n+  env = config['environment']\n+  FILE_TEST_LIST = os.path.join(env['SRCDIR'], 'src', 'test', 'test_list.txt')\n+  DEFAULT_BINARY = os.path.join(env['BUILDDIR'],'src', 'test','test_bitcoin'+env['EXEEXT'])\n+  parser = default_options_parser(DEFAULT_BINARY, FILE_TEST_LIST)\n+  (options, binaries) = parser.parse_args()\n+  if options.write_list:\n+      write_test_list(options.binary, FILE_TEST_LIST)\n+      sys.exit(0)\n+  test_list = read_test_list(FILE_TEST_LIST)\n+  binaries = [options.binary]\n+# Bitcoin Core specific options parsing: end\n+\n+  if (options.output_dir is not None and\n+      not os.path.isdir(options.output_dir)):\n+    parser.error('--output_dir value must be an existing directory, '\n+                 'current value is \"%s\"' % options.output_dir)\n+\n+  # Append gtest-parallel-logs to log output, this is to avoid deleting user\n+  # data if an user passes a directory where files are already present. If a\n+  # user specifies --output_dir=Docs/, we'll create Docs/gtest-parallel-logs\n+  # and clean that directory out on startup, instead of nuking Docs/.\n+  if options.output_dir:\n+    options.output_dir = os.path.join(options.output_dir,\n+                                      'gtest-parallel-logs')\n+\n+  if binaries == []:\n+    parser.print_usage()\n+    sys.exit(1)\n+\n+  if options.shard_count < 1:",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178236641",
      "id" : 178236641,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 737,
      "path" : "src/test/parallel.py",
      "position" : 737,
      "pull_request_review_id" : 108267828,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T05:52:47Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178236641",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178237305"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178237305"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "Not sure it makes sense to give `shard_index` a default. I think you want to ensure that `shard_count` and `shard_index` are used in combination, so if `shard_count` is used and `options.shard_index` is `None`,  you print proper usage. Right now `options.shard_index` will just default to 0, so you can't tell if it's being used properly with `shard_count`. ",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T06:01:30Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import configparser\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n+\n+    self.output_dir = output_dir\n+\n+    self.total_tasks = 0\n+    self.finished_tasks = 0\n+    self.out = Outputter(sys.stdout)\n+    self.stdout_lock = threading.Lock()\n+\n+  def move_to(self, destination_dir, tasks):\n+    if self.output_dir is None:\n+      return\n+\n+    destination_dir = os.path.join(self.output_dir, destination_dir)\n+    os.makedirs(destination_dir)\n+    for task in tasks:\n+      shutil.move(task.log_file, destination_dir)\n+\n+  def print_tests(self, message, tasks, print_try_number):\n+    self.out.permanent_line(\"%s (%s/%s):\" %\n+                            (message, len(tasks), self.total_tasks))\n+    for task in sorted(tasks):\n+      runtime_ms = 'Interrupted'\n+      if task.runtime_ms is not None:\n+        runtime_ms = '%d ms' % task.runtime_ms\n+      self.out.permanent_line(\"%11s: %s %s%s\" % (\n+          runtime_ms, task.test_binary, task.test_name,\n+          (\" (try #%d)\" % task.execution_number) if print_try_number else \"\"))\n+\n+  def log_exit(self, task):\n+    with self.stdout_lock:\n+      self.finished_tasks += 1\n+      self.out.transient_line(\"[%d/%d] %s (%d ms)\"\n+                              % (self.finished_tasks, self.total_tasks,\n+                                 task.test_name, task.runtime_ms))\n+      if task.exit_code != 0:\n+        with open(task.log_file) as f:\n+          for line in f.readlines():\n+            self.out.permanent_line(line.rstrip())\n+        self.out.permanent_line(\n+          \"[%d/%d] %s returned/aborted with exit code %d (%d ms)\"\n+          % (self.finished_tasks, self.total_tasks, task.test_name,\n+             task.exit_code, task.runtime_ms))\n+\n+    if self.output_dir is None:\n+      # Try to remove the file 100 times (sleeping for 0.1 second in between).\n+      # This is a workaround for a process handle seemingly holding on to the\n+      # file for too long inside os.subprocess. This workaround is in place\n+      # until we figure out a minimal repro to report upstream (or a better\n+      # suspect) to prevent os.remove exceptions.\n+      num_tries = 100\n+      for i in range(num_tries):\n+        try:\n+          os.remove(task.log_file)\n+        except OSError as e:\n+          if e.errno is not errno.ENOENT:\n+            if i is num_tries - 1:\n+              self.out.permanent_line('Could not remove temporary log file: ' + str(e))\n+            else:\n+              time.sleep(0.1)\n+            continue\n+        break\n+\n+  def log_tasks(self, total_tasks):\n+    self.total_tasks += total_tasks\n+    self.out.transient_line(\"[0/%d] Running tests...\" % self.total_tasks)\n+\n+  def summarize(self, passed_tasks, failed_tasks, interrupted_tasks):\n+    stats = {}\n+    def add_stats(stats, task, idx):\n+      task_key = (task.test_binary, task.test_name)\n+      if not task_key in stats:\n+        # (passed, failed, interrupted) task_key is added as tie breaker to get\n+        # alphabetic sorting on equally-stable tests\n+        stats[task_key] = [0, 0, 0, task_key]\n+      stats[task_key][idx] += 1\n+\n+    for task in passed_tasks:\n+      add_stats(stats, task, 0)\n+    for task in failed_tasks:\n+      add_stats(stats, task, 1)\n+    for task in interrupted_tasks:\n+      add_stats(stats, task, 2)\n+\n+    self.out.permanent_line(\"SUMMARY:\")\n+    for task_key in sorted(stats, key=stats.__getitem__):\n+      (num_passed, num_failed, num_interrupted, _) = stats[task_key]\n+      (test_binary, task_name) = task_key\n+      self.out.permanent_line(\n+          \"  %s %s passed %d / %d times%s.\" %\n+              (test_binary, task_name, num_passed,\n+               num_passed + num_failed + num_interrupted,\n+               \"\" if num_interrupted == 0 else (\" (%d interrupted)\" % num_interrupted)))\n+\n+  def flush(self):\n+    self.out.flush_transient_output()\n+\n+\n+class CollectTestResults(object):\n+  def __init__(self, json_dump_filepath):\n+    self.test_results_lock = threading.Lock()\n+    self.json_dump_file = open(json_dump_filepath, 'w')\n+    self.test_results = {\n+        \"interrupted\": False,\n+        \"path_delimiter\": \".\",\n+        # Third version of the file format. See the link in the flag description\n+        # for details.\n+        \"version\": 3,\n+        \"seconds_since_epoch\": int(time.time()),\n+        \"num_failures_by_type\": {\n+            \"PASS\": 0,\n+            \"FAIL\": 0,\n+        },\n+        \"tests\": {},\n+    }\n+\n+  def log(self, test, runtime_ms, actual_result):\n+    with self.test_results_lock:\n+      self.test_results['num_failures_by_type'][actual_result] += 1\n+      results = self.test_results['tests']\n+      for name in test.split('.'):\n+        results = results.setdefault(name, {})\n+\n+      if results:\n+        results['actual'] += ' ' + actual_result\n+        results['times'].append(runtime_ms)\n+      else:  # This is the first invocation of the test\n+        results['actual'] = actual_result\n+        results['times'] = [runtime_ms]\n+        results['time'] = runtime_ms\n+        results['expected'] = 'PASS'\n+\n+  def dump_to_file_and_close(self):\n+    json.dump(self.test_results, self.json_dump_file)\n+    self.json_dump_file.close()\n+\n+\n+# Record of test runtimes. Has built-in locking.\n+class TestTimes(object):\n+  class LockedFile(object):\n+    def __init__(self, filename, mode):\n+      self._filename = filename\n+      self._mode = mode\n+      self._fo = None\n+\n+    def __enter__(self):\n+      self._fo = open(self._filename, self._mode)\n+\n+      # Regardless of opening mode we always seek to the beginning of file.\n+      # This simplifies code working with LockedFile and also ensures that\n+      # we lock (and unlock below) always the same region in file on win32.\n+      self._fo.seek(0)\n+\n+      try:\n+        if sys.platform == 'win32':\n+          # We are locking here fixed location in file to use it as\n+          # an exclusive lock on entire file.\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_LOCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_EX)\n+      except IOError:\n+        self._fo.close()\n+        raise\n+\n+      return self._fo\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+      # Flush any buffered data to disk. This is needed to prevent race\n+      # condition which happens from the moment of releasing file lock\n+      # till closing the file.\n+      self._fo.flush()\n+\n+      try:\n+        if sys.platform == 'win32':\n+          self._fo.seek(0)\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_UNLCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_UN)\n+      finally:\n+        self._fo.close()\n+\n+      return exc_value is None\n+\n+  def __init__(self, save_file):\n+    \"Create new object seeded with saved test times from the given file.\"\n+    self.__times = {}  # (test binary, test name) -> runtime in ms\n+\n+    # Protects calls to record_test_time(); other calls are not\n+    # expected to be made concurrently.\n+    self.__lock = threading.Lock()\n+\n+    try:\n+      with TestTimes.LockedFile(save_file, 'rb') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+    except IOError:\n+      # We couldn't obtain the lock.\n+      return\n+\n+    # Discard saved times if the format isn't right.\n+    if type(times) is not dict:\n+      return\n+    for ((test_binary, test_name), runtime) in list(times.items()):\n+      if (type(test_binary) is not str or type(test_name) is not str\n+          or type(runtime) not in {int, long, type(None)}):\n+        return\n+\n+    self.__times = times\n+\n+  def get_test_time(self, binary, testname):\n+    \"\"\"Return the last duration for the given test as an integer number of\n+    milliseconds, or None if the test failed or if there's no record for it.\"\"\"\n+    return self.__times.get((binary, testname), None)\n+\n+  def record_test_time(self, binary, testname, runtime_ms):\n+    \"\"\"Record that the given test ran in the specified number of\n+    milliseconds. If the test failed, runtime_ms should be None.\"\"\"\n+    with self.__lock:\n+      self.__times[(binary, testname)] = runtime_ms\n+\n+  def write_to_file(self, save_file):\n+    \"Write all the times to file.\"\n+    try:\n+      with TestTimes.LockedFile(save_file, 'a+b') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+\n+        if times is None:\n+          times = self.__times\n+        else:\n+          times.update(self.__times)\n+\n+        # We erase data from file while still holding a lock to it. This\n+        # way reading old test times and appending new ones are atomic\n+        # for external viewer.\n+        fd.seek(0)\n+        fd.truncate()\n+        with gzip.GzipFile(fileobj=fd, mode='wb') as gzf:\n+          cPickle.dump(times, gzf, PICKLE_HIGHEST_PROTOCOL)\n+    except IOError:\n+      pass  # ignore errors---saving the times isn't that important\n+\n+  @staticmethod\n+  def __read_test_times_file(fd):\n+    try:\n+      with gzip.GzipFile(fileobj=fd, mode='rb') as gzf:\n+        times = cPickle.load(gzf)\n+    except Exception:\n+      # File doesn't exist, isn't readable, is malformed---whatever.\n+      # Just ignore it.\n+      return None\n+    else:\n+      return times\n+\n+\n+def find_tests(test_list, binaries, additional_args, options, times):\n+  test_count = 0\n+  tasks = []\n+  for test_binary in binaries:\n+    command = [test_binary]\n+    command += additional_args\n+\n+    for test_name in test_list:\n+      last_execution_time = times.get_test_time(test_binary, test_name)\n+      if options.failed and last_execution_time is not None:\n+        continue\n+\n+      test_command = command + ['--run_test=' + test_name]\n+      if (test_count - options.shard_index) % options.shard_count == 0:\n+        for execution_number in range(options.repeat):\n+          tasks.append(Task(test_binary, test_name, test_command,\n+                            execution_number + 1, last_execution_time,\n+                            options.output_dir))\n+\n+      test_count += 1\n+\n+  # Sort the tasks to run the slowest tests first, so that faster ones can be\n+  # finished in parallel.\n+  return sorted(tasks, reverse=True)\n+\n+\n+def execute_tasks(tasks, pool_size, task_manager,\n+                  timeout, serialize_test_cases):\n+  class WorkerFn(object):\n+    def __init__(self, tasks, running_groups):\n+      self.tasks = tasks\n+      self.running_groups = running_groups\n+      self.task_lock = threading.Lock()\n+\n+    def __call__(self):\n+      while True:\n+        with self.task_lock:\n+          for task_id in range(len(self.tasks)):\n+            task = self.tasks[task_id]\n+\n+            if self.running_groups is not None:\n+              test_group = task.test_name.split('.')[0]\n+              if test_group in self.running_groups:\n+                # Try to find other non-running test group.\n+                continue\n+              else:\n+                self.running_groups.add(test_group)\n+\n+            del self.tasks[task_id]\n+            break\n+          else:\n+            # Either there is no tasks left or number or remaining test\n+            # cases (groups) is less than number or running threads.\n+            return\n+\n+        task_manager.run_task(task)\n+\n+        if self.running_groups is not None:\n+          with self.task_lock:\n+            self.running_groups.remove(test_group)\n+\n+  def start_daemon(func):\n+    t = threading.Thread(target=func)\n+    t.daemon = True\n+    t.start()\n+    return t\n+\n+  try:\n+    if timeout:\n+      timeout.start()\n+    running_groups = set() if serialize_test_cases else None\n+    worker_fn = WorkerFn(tasks, running_groups)\n+    workers = [start_daemon(worker_fn) for _ in range(pool_size)]\n+    for worker in workers:\n+      worker.join()\n+  finally:\n+    if timeout:\n+      timeout.cancel()\n+\n+\n+def default_options_parser(default_binary, file_test_list):\n+  parser = optparse.OptionParser(\n+      usage = 'usage: %prog [options] binary [binary ...] -- [additional args]')\n+\n+  parser.add_option('-d', '--output_dir', type='string', default=None,\n+                    help='Output directory for test logs. Logs will be '\n+                         'available under gtest-parallel-logs/, so '\n+                         '--output_dir=/tmp will results in all logs being '\n+                         'available under /tmp/gtest-parallel-logs/.')\n+  parser.add_option('-r', '--repeat', type='int', default=1,\n+                    help='Number of times to execute all the tests.')\n+  parser.add_option('--retry_failed', type='int', default=0,\n+                    help='Number of times to repeat failed tests.')\n+  parser.add_option('--failed', action='store_true', default=False,\n+                    help='run only failed and new tests')\n+  parser.add_option('-w', '--workers', type='int',\n+                    default=multiprocessing.cpu_count(),\n+                    help='number of workers to spawn')\n+  parser.add_option('--gtest_color', type='string', default='yes',\n+                    help='color output')\n+  parser.add_option('--gtest_filter', type='string', default='',\n+                    help='test filter')\n+  parser.add_option('--gtest_also_run_disabled_tests', action='store_true',\n+                    default=False, help='run disabled tests too')\n+  parser.add_option('--print_test_times', action='store_true', default=False,\n+                    help='list the run time of each test at the end of execution')\n+  parser.add_option('--shard_count', type='int', default=1,\n+                    help='total number of shards (for sharding test execution '\n+                         'between multiple machines)')\n+  parser.add_option('--shard_index', type='int', default=0,",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178237305",
      "id" : 178237305,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 672,
      "path" : "src/test/parallel.py",
      "position" : 672,
      "pull_request_review_id" : 108268589,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T06:01:30Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178237305",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178238524"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178238524"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "Should this be `split('/')[0]` ?\r\n\r\nRight now `task.test_name` is a line from `src/test/test_list.txt` like \r\n```\r\nbloom_tests/rolling_bloom\r\n```\r\nSo the split will always return that full path, since there is no `.` - making `option.serialize_test_cases` just run everything in parallel anyway. ",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T06:15:14Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import configparser\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n+\n+    self.output_dir = output_dir\n+\n+    self.total_tasks = 0\n+    self.finished_tasks = 0\n+    self.out = Outputter(sys.stdout)\n+    self.stdout_lock = threading.Lock()\n+\n+  def move_to(self, destination_dir, tasks):\n+    if self.output_dir is None:\n+      return\n+\n+    destination_dir = os.path.join(self.output_dir, destination_dir)\n+    os.makedirs(destination_dir)\n+    for task in tasks:\n+      shutil.move(task.log_file, destination_dir)\n+\n+  def print_tests(self, message, tasks, print_try_number):\n+    self.out.permanent_line(\"%s (%s/%s):\" %\n+                            (message, len(tasks), self.total_tasks))\n+    for task in sorted(tasks):\n+      runtime_ms = 'Interrupted'\n+      if task.runtime_ms is not None:\n+        runtime_ms = '%d ms' % task.runtime_ms\n+      self.out.permanent_line(\"%11s: %s %s%s\" % (\n+          runtime_ms, task.test_binary, task.test_name,\n+          (\" (try #%d)\" % task.execution_number) if print_try_number else \"\"))\n+\n+  def log_exit(self, task):\n+    with self.stdout_lock:\n+      self.finished_tasks += 1\n+      self.out.transient_line(\"[%d/%d] %s (%d ms)\"\n+                              % (self.finished_tasks, self.total_tasks,\n+                                 task.test_name, task.runtime_ms))\n+      if task.exit_code != 0:\n+        with open(task.log_file) as f:\n+          for line in f.readlines():\n+            self.out.permanent_line(line.rstrip())\n+        self.out.permanent_line(\n+          \"[%d/%d] %s returned/aborted with exit code %d (%d ms)\"\n+          % (self.finished_tasks, self.total_tasks, task.test_name,\n+             task.exit_code, task.runtime_ms))\n+\n+    if self.output_dir is None:\n+      # Try to remove the file 100 times (sleeping for 0.1 second in between).\n+      # This is a workaround for a process handle seemingly holding on to the\n+      # file for too long inside os.subprocess. This workaround is in place\n+      # until we figure out a minimal repro to report upstream (or a better\n+      # suspect) to prevent os.remove exceptions.\n+      num_tries = 100\n+      for i in range(num_tries):\n+        try:\n+          os.remove(task.log_file)\n+        except OSError as e:\n+          if e.errno is not errno.ENOENT:\n+            if i is num_tries - 1:\n+              self.out.permanent_line('Could not remove temporary log file: ' + str(e))\n+            else:\n+              time.sleep(0.1)\n+            continue\n+        break\n+\n+  def log_tasks(self, total_tasks):\n+    self.total_tasks += total_tasks\n+    self.out.transient_line(\"[0/%d] Running tests...\" % self.total_tasks)\n+\n+  def summarize(self, passed_tasks, failed_tasks, interrupted_tasks):\n+    stats = {}\n+    def add_stats(stats, task, idx):\n+      task_key = (task.test_binary, task.test_name)\n+      if not task_key in stats:\n+        # (passed, failed, interrupted) task_key is added as tie breaker to get\n+        # alphabetic sorting on equally-stable tests\n+        stats[task_key] = [0, 0, 0, task_key]\n+      stats[task_key][idx] += 1\n+\n+    for task in passed_tasks:\n+      add_stats(stats, task, 0)\n+    for task in failed_tasks:\n+      add_stats(stats, task, 1)\n+    for task in interrupted_tasks:\n+      add_stats(stats, task, 2)\n+\n+    self.out.permanent_line(\"SUMMARY:\")\n+    for task_key in sorted(stats, key=stats.__getitem__):\n+      (num_passed, num_failed, num_interrupted, _) = stats[task_key]\n+      (test_binary, task_name) = task_key\n+      self.out.permanent_line(\n+          \"  %s %s passed %d / %d times%s.\" %\n+              (test_binary, task_name, num_passed,\n+               num_passed + num_failed + num_interrupted,\n+               \"\" if num_interrupted == 0 else (\" (%d interrupted)\" % num_interrupted)))\n+\n+  def flush(self):\n+    self.out.flush_transient_output()\n+\n+\n+class CollectTestResults(object):\n+  def __init__(self, json_dump_filepath):\n+    self.test_results_lock = threading.Lock()\n+    self.json_dump_file = open(json_dump_filepath, 'w')\n+    self.test_results = {\n+        \"interrupted\": False,\n+        \"path_delimiter\": \".\",\n+        # Third version of the file format. See the link in the flag description\n+        # for details.\n+        \"version\": 3,\n+        \"seconds_since_epoch\": int(time.time()),\n+        \"num_failures_by_type\": {\n+            \"PASS\": 0,\n+            \"FAIL\": 0,\n+        },\n+        \"tests\": {},\n+    }\n+\n+  def log(self, test, runtime_ms, actual_result):\n+    with self.test_results_lock:\n+      self.test_results['num_failures_by_type'][actual_result] += 1\n+      results = self.test_results['tests']\n+      for name in test.split('.'):\n+        results = results.setdefault(name, {})\n+\n+      if results:\n+        results['actual'] += ' ' + actual_result\n+        results['times'].append(runtime_ms)\n+      else:  # This is the first invocation of the test\n+        results['actual'] = actual_result\n+        results['times'] = [runtime_ms]\n+        results['time'] = runtime_ms\n+        results['expected'] = 'PASS'\n+\n+  def dump_to_file_and_close(self):\n+    json.dump(self.test_results, self.json_dump_file)\n+    self.json_dump_file.close()\n+\n+\n+# Record of test runtimes. Has built-in locking.\n+class TestTimes(object):\n+  class LockedFile(object):\n+    def __init__(self, filename, mode):\n+      self._filename = filename\n+      self._mode = mode\n+      self._fo = None\n+\n+    def __enter__(self):\n+      self._fo = open(self._filename, self._mode)\n+\n+      # Regardless of opening mode we always seek to the beginning of file.\n+      # This simplifies code working with LockedFile and also ensures that\n+      # we lock (and unlock below) always the same region in file on win32.\n+      self._fo.seek(0)\n+\n+      try:\n+        if sys.platform == 'win32':\n+          # We are locking here fixed location in file to use it as\n+          # an exclusive lock on entire file.\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_LOCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_EX)\n+      except IOError:\n+        self._fo.close()\n+        raise\n+\n+      return self._fo\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+      # Flush any buffered data to disk. This is needed to prevent race\n+      # condition which happens from the moment of releasing file lock\n+      # till closing the file.\n+      self._fo.flush()\n+\n+      try:\n+        if sys.platform == 'win32':\n+          self._fo.seek(0)\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_UNLCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_UN)\n+      finally:\n+        self._fo.close()\n+\n+      return exc_value is None\n+\n+  def __init__(self, save_file):\n+    \"Create new object seeded with saved test times from the given file.\"\n+    self.__times = {}  # (test binary, test name) -> runtime in ms\n+\n+    # Protects calls to record_test_time(); other calls are not\n+    # expected to be made concurrently.\n+    self.__lock = threading.Lock()\n+\n+    try:\n+      with TestTimes.LockedFile(save_file, 'rb') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+    except IOError:\n+      # We couldn't obtain the lock.\n+      return\n+\n+    # Discard saved times if the format isn't right.\n+    if type(times) is not dict:\n+      return\n+    for ((test_binary, test_name), runtime) in list(times.items()):\n+      if (type(test_binary) is not str or type(test_name) is not str\n+          or type(runtime) not in {int, long, type(None)}):\n+        return\n+\n+    self.__times = times\n+\n+  def get_test_time(self, binary, testname):\n+    \"\"\"Return the last duration for the given test as an integer number of\n+    milliseconds, or None if the test failed or if there's no record for it.\"\"\"\n+    return self.__times.get((binary, testname), None)\n+\n+  def record_test_time(self, binary, testname, runtime_ms):\n+    \"\"\"Record that the given test ran in the specified number of\n+    milliseconds. If the test failed, runtime_ms should be None.\"\"\"\n+    with self.__lock:\n+      self.__times[(binary, testname)] = runtime_ms\n+\n+  def write_to_file(self, save_file):\n+    \"Write all the times to file.\"\n+    try:\n+      with TestTimes.LockedFile(save_file, 'a+b') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+\n+        if times is None:\n+          times = self.__times\n+        else:\n+          times.update(self.__times)\n+\n+        # We erase data from file while still holding a lock to it. This\n+        # way reading old test times and appending new ones are atomic\n+        # for external viewer.\n+        fd.seek(0)\n+        fd.truncate()\n+        with gzip.GzipFile(fileobj=fd, mode='wb') as gzf:\n+          cPickle.dump(times, gzf, PICKLE_HIGHEST_PROTOCOL)\n+    except IOError:\n+      pass  # ignore errors---saving the times isn't that important\n+\n+  @staticmethod\n+  def __read_test_times_file(fd):\n+    try:\n+      with gzip.GzipFile(fileobj=fd, mode='rb') as gzf:\n+        times = cPickle.load(gzf)\n+    except Exception:\n+      # File doesn't exist, isn't readable, is malformed---whatever.\n+      # Just ignore it.\n+      return None\n+    else:\n+      return times\n+\n+\n+def find_tests(test_list, binaries, additional_args, options, times):\n+  test_count = 0\n+  tasks = []\n+  for test_binary in binaries:\n+    command = [test_binary]\n+    command += additional_args\n+\n+    for test_name in test_list:\n+      last_execution_time = times.get_test_time(test_binary, test_name)\n+      if options.failed and last_execution_time is not None:\n+        continue\n+\n+      test_command = command + ['--run_test=' + test_name]\n+      if (test_count - options.shard_index) % options.shard_count == 0:\n+        for execution_number in range(options.repeat):\n+          tasks.append(Task(test_binary, test_name, test_command,\n+                            execution_number + 1, last_execution_time,\n+                            options.output_dir))\n+\n+      test_count += 1\n+\n+  # Sort the tasks to run the slowest tests first, so that faster ones can be\n+  # finished in parallel.\n+  return sorted(tasks, reverse=True)\n+\n+\n+def execute_tasks(tasks, pool_size, task_manager,\n+                  timeout, serialize_test_cases):\n+  class WorkerFn(object):\n+    def __init__(self, tasks, running_groups):\n+      self.tasks = tasks\n+      self.running_groups = running_groups\n+      self.task_lock = threading.Lock()\n+\n+    def __call__(self):\n+      while True:\n+        with self.task_lock:\n+          for task_id in range(len(self.tasks)):\n+            task = self.tasks[task_id]\n+\n+            if self.running_groups is not None:\n+              test_group = task.test_name.split('.')[0]",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178238524",
      "id" : 178238524,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 604,
      "path" : "src/test/parallel.py",
      "position" : 604,
      "pull_request_review_id" : 108269924,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T06:15:14Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178238524",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "_links" : {
         "html" : {
            "href" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178238820"
         },
         "pull_request" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831"
         },
         "self" : {
            "href" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178238820"
         }
      },
      "author_association" : "CONTRIBUTOR",
      "body" : "I think this option is currently incompatible with sharding, since it shards tests in a round-robin type fashion, thus splitting up test_groups between shards. This is probably desired, but just need to document it, or try to prevent the two options being used in combination.",
      "commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "created_at" : "2018-03-30T06:19:11Z",
      "diff_hunk" : "@@ -0,0 +1,864 @@\n+#!/usr/bin/env python\n+#\n+# Copyright 2013 Google Inc. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+try:\n+    import _pickle as cPickle\n+except ImportError:\n+    import cPickle\n+import errno\n+from functools import total_ordering\n+import gzip\n+import json\n+import multiprocessing\n+import configparser\n+import optparse\n+import os\n+from pickle import HIGHEST_PROTOCOL as PICKLE_HIGHEST_PROTOCOL\n+import re\n+import shutil\n+import signal\n+import subprocess\n+import sys\n+import tempfile\n+try:\n+    import _thread as thread\n+except ImportError:\n+    import thread\n+import threading\n+import time\n+\n+if sys.version_info.major >= 3:\n+    long = int\n+\n+if sys.platform == 'win32':\n+  import msvcrt\n+else:\n+  import fcntl\n+\n+\n+# An object that catches SIGINT sent to the Python process and notices\n+# if processes passed to wait() die by SIGINT (we need to look for\n+# both of those cases, because pressing Ctrl+C can result in either\n+# the main process or one of the subprocesses getting the signal).\n+#\n+# Before a SIGINT is seen, wait(p) will simply call p.wait() and\n+# return the result. Once a SIGINT has been seen (in the main process\n+# or a subprocess, including the one the current call is waiting for),\n+# wait(p) will call p.terminate() and raise ProcessWasInterrupted.\n+class SigintHandler(object):\n+  class ProcessWasInterrupted(Exception): pass\n+  sigint_returncodes = {-signal.SIGINT,  # Unix\n+                        -1073741510,     # Windows\n+                        }\n+  def __init__(self):\n+    self.__lock = threading.Lock()\n+    self.__processes = set()\n+    self.__got_sigint = False\n+    signal.signal(signal.SIGINT, lambda signal_num, frame: self.interrupt())\n+  def __on_sigint(self):\n+    self.__got_sigint = True\n+    while self.__processes:\n+      try:\n+        self.__processes.pop().terminate()\n+      except OSError:\n+        pass\n+  def interrupt(self):\n+    with self.__lock:\n+      self.__on_sigint()\n+  def got_sigint(self):\n+    with self.__lock:\n+      return self.__got_sigint\n+  def wait(self, p):\n+    with self.__lock:\n+      if self.__got_sigint:\n+        p.terminate()\n+      self.__processes.add(p)\n+    code = p.wait()\n+    with self.__lock:\n+      self.__processes.discard(p)\n+      if code in self.sigint_returncodes:\n+        self.__on_sigint()\n+      if self.__got_sigint:\n+        raise self.ProcessWasInterrupted\n+    return code\n+sigint_handler = SigintHandler()\n+\n+\n+# Return the width of the terminal, or None if it couldn't be\n+# determined (e.g. because we're not being run interactively).\n+def term_width(out):\n+  if not out.isatty():\n+    return None\n+  try:\n+    p = subprocess.Popen([\"stty\", \"size\"],\n+                         stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n+    (out, err) = p.communicate()\n+    if p.returncode != 0 or err:\n+      return None\n+    return int(out.split()[1])\n+  except (IndexError, OSError, ValueError):\n+    return None\n+\n+\n+# Output transient and permanent lines of text. If several transient\n+# lines are written in sequence, the new will overwrite the old. We\n+# use this to ensure that lots of unimportant info (tests passing)\n+# won't drown out important info (tests failing).\n+class Outputter(object):\n+  def __init__(self, out_file):\n+    self.__out_file = out_file\n+    self.__previous_line_was_transient = False\n+    self.__width = term_width(out_file)  # Line width, or None if not a tty.\n+  def transient_line(self, msg):\n+    if self.__width is None:\n+      self.__out_file.write(msg + \"\\n\")\n+    else:\n+      self.__out_file.write(\"\\r\" + msg[:self.__width].ljust(self.__width))\n+      self.__previous_line_was_transient = True\n+  def flush_transient_output(self):\n+    if self.__previous_line_was_transient:\n+      self.__out_file.write(\"\\n\")\n+      self.__previous_line_was_transient = False\n+  def permanent_line(self, msg):\n+    self.flush_transient_output()\n+    self.__out_file.write(msg + \"\\n\")\n+\n+\n+def get_save_file_path():\n+  \"\"\"Return path to file for saving transient data.\"\"\"\n+  if sys.platform == 'win32':\n+    default_cache_path = os.path.join(os.path.expanduser('~'),\n+                                      'AppData', 'Local')\n+    cache_path = os.environ.get('LOCALAPPDATA', default_cache_path)\n+  else:\n+    # We don't use xdg module since it's not a standard.\n+    default_cache_path = os.path.join(os.path.expanduser('~'), '.cache')\n+    cache_path = os.environ.get('XDG_CACHE_HOME', default_cache_path)\n+\n+  if os.path.isdir(cache_path):\n+    return os.path.join(cache_path, 'gtest-parallel')\n+  else:\n+    sys.stderr.write('Directory {} does not exist'.format(cache_path))\n+    return os.path.join(os.path.expanduser('~'), '.gtest-parallel-times')\n+\n+\n+@total_ordering\n+class Task(object):\n+  \"\"\"Stores information about a task (single execution of a test).\n+\n+  This class stores information about the test to be executed (gtest binary and\n+  test name), and its result (log file, exit code and runtime).\n+  Each task is uniquely identified by the gtest binary, the test name and an\n+  execution number that increases each time the test is executed.\n+  Additionaly we store the last execution time, so that next time the test is\n+  executed, the slowest tests are run first.\n+  \"\"\"\n+  def __init__(self, test_binary, test_name, test_command, execution_number,\n+               last_execution_time, output_dir):\n+    self.test_name = test_name\n+    self.output_dir = output_dir\n+    self.test_binary = test_binary\n+    self.test_command = test_command\n+    self.execution_number = execution_number\n+    self.last_execution_time = last_execution_time\n+\n+    self.exit_code = None\n+    self.runtime_ms = None\n+\n+    self.test_id = (test_binary, test_name)\n+    self.task_id = (test_binary, test_name, self.execution_number)\n+\n+    self.log_file = Task._logname(self.output_dir, self.test_binary,\n+                                  test_name, self.execution_number)\n+\n+  def __sorting_key(self):\n+    # Unseen or failing tests (both missing execution time) take precedence over\n+    # execution time. Tests are greater (seen as slower) when missing times so\n+    # that they are executed first.\n+    return (1 if self.last_execution_time is None else 0,\n+            self.last_execution_time)\n+\n+  def __eq__(self, other):\n+      return self.__sorting_key() == other.__sorting_key()\n+\n+  def __ne__(self, other):\n+      return not (self == other)\n+\n+  def __lt__(self, other):\n+      return self.__sorting_key() < other.__sorting_key()\n+\n+  @staticmethod\n+  def _normalize(string):\n+    return re.sub('[^A-Za-z0-9]', '_', string)\n+\n+  @staticmethod\n+  def _logname(output_dir, test_binary, test_name, execution_number):\n+    # Store logs to temporary files if there is no output_dir.\n+    if output_dir is None:\n+      (log_handle, log_name) = tempfile.mkstemp(prefix='gtest_parallel_',\n+                                                suffix=\".log\")\n+      os.close(log_handle)\n+      return log_name\n+\n+    log_name = '%s-%s-%d.log' % (Task._normalize(os.path.basename(test_binary)),\n+                                 Task._normalize(test_name), execution_number)\n+\n+    return os.path.join(output_dir, log_name)\n+\n+  def run(self):\n+    begin = time.time()\n+    with open(self.log_file, 'w') as log:\n+      task = subprocess.Popen(self.test_command, stdout=log, stderr=log)\n+      try:\n+        self.exit_code = sigint_handler.wait(task)\n+      except sigint_handler.ProcessWasInterrupted:\n+        thread.exit()\n+    self.runtime_ms = int(1000 * (time.time() - begin))\n+    self.last_execution_time = None if self.exit_code else self.runtime_ms\n+\n+\n+class TaskManager(object):\n+  \"\"\"Executes the tasks and stores the passed, failed and interrupted tasks.\n+\n+  When a task is run, this class keeps track if it passed, failed or was\n+  interrupted. After a task finishes it calls the relevant functions of the\n+  Logger, TestResults and TestTimes classes, and in case of failure, retries the\n+  test as specified by the --retry_failed flag.\n+  \"\"\"\n+  def __init__(self, times, logger, test_results, task_factory, times_to_retry,\n+               initial_execution_number):\n+    self.times = times\n+    self.logger = logger\n+    self.test_results = test_results\n+    self.task_factory = task_factory\n+    self.times_to_retry = times_to_retry\n+    self.initial_execution_number = initial_execution_number\n+\n+    self.global_exit_code = 0\n+\n+    self.passed = []\n+    self.failed = []\n+    self.started = {}\n+    self.execution_number = {}\n+\n+    self.lock = threading.Lock()\n+\n+  def __get_next_execution_number(self, test_id):\n+    with self.lock:\n+      next_execution_number = self.execution_number.setdefault(\n+          test_id, self.initial_execution_number)\n+      self.execution_number[test_id] += 1\n+    return next_execution_number\n+\n+  def __register_start(self, task):\n+    with self.lock:\n+      self.started[task.task_id] = task\n+\n+  def __register_exit(self, task):\n+    self.logger.log_exit(task)\n+    self.times.record_test_time(task.test_binary, task.test_name,\n+                                task.last_execution_time)\n+    if self.test_results:\n+      self.test_results.log(task.test_name, task.runtime_ms,\n+                            \"PASS\" if task.exit_code == 0 else \"FAIL\")\n+\n+    with self.lock:\n+      self.started.pop(task.task_id)\n+      if task.exit_code == 0:\n+        self.passed.append(task)\n+      else:\n+        self.failed.append(task)\n+\n+  def run_task(self, task):\n+    for try_number in range(self.times_to_retry + 1):\n+      self.__register_start(task)\n+      task.run()\n+      self.__register_exit(task)\n+\n+      if task.exit_code == 0:\n+        break\n+\n+      if try_number < self.times_to_retry:\n+        execution_number = self.__get_next_execution_number(task.test_id)\n+        # We need create a new Task instance. Each task represents a single test\n+        # execution, with its own runtime, exit code and log file.\n+        task = self.task_factory(task.test_binary, task.test_name,\n+                                 task.test_command, execution_number,\n+                                 task.last_execution_time, task.output_dir)\n+\n+    with self.lock:\n+      if task.exit_code != 0:\n+        self.global_exit_code = task.exit_code\n+\n+\n+class FilterFormat(object):\n+  def __init__(self, output_dir):\n+    if sys.stdout.isatty():\n+      # stdout needs to be unbuffered since the output is interactive.\n+      sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)\n+\n+    self.output_dir = output_dir\n+\n+    self.total_tasks = 0\n+    self.finished_tasks = 0\n+    self.out = Outputter(sys.stdout)\n+    self.stdout_lock = threading.Lock()\n+\n+  def move_to(self, destination_dir, tasks):\n+    if self.output_dir is None:\n+      return\n+\n+    destination_dir = os.path.join(self.output_dir, destination_dir)\n+    os.makedirs(destination_dir)\n+    for task in tasks:\n+      shutil.move(task.log_file, destination_dir)\n+\n+  def print_tests(self, message, tasks, print_try_number):\n+    self.out.permanent_line(\"%s (%s/%s):\" %\n+                            (message, len(tasks), self.total_tasks))\n+    for task in sorted(tasks):\n+      runtime_ms = 'Interrupted'\n+      if task.runtime_ms is not None:\n+        runtime_ms = '%d ms' % task.runtime_ms\n+      self.out.permanent_line(\"%11s: %s %s%s\" % (\n+          runtime_ms, task.test_binary, task.test_name,\n+          (\" (try #%d)\" % task.execution_number) if print_try_number else \"\"))\n+\n+  def log_exit(self, task):\n+    with self.stdout_lock:\n+      self.finished_tasks += 1\n+      self.out.transient_line(\"[%d/%d] %s (%d ms)\"\n+                              % (self.finished_tasks, self.total_tasks,\n+                                 task.test_name, task.runtime_ms))\n+      if task.exit_code != 0:\n+        with open(task.log_file) as f:\n+          for line in f.readlines():\n+            self.out.permanent_line(line.rstrip())\n+        self.out.permanent_line(\n+          \"[%d/%d] %s returned/aborted with exit code %d (%d ms)\"\n+          % (self.finished_tasks, self.total_tasks, task.test_name,\n+             task.exit_code, task.runtime_ms))\n+\n+    if self.output_dir is None:\n+      # Try to remove the file 100 times (sleeping for 0.1 second in between).\n+      # This is a workaround for a process handle seemingly holding on to the\n+      # file for too long inside os.subprocess. This workaround is in place\n+      # until we figure out a minimal repro to report upstream (or a better\n+      # suspect) to prevent os.remove exceptions.\n+      num_tries = 100\n+      for i in range(num_tries):\n+        try:\n+          os.remove(task.log_file)\n+        except OSError as e:\n+          if e.errno is not errno.ENOENT:\n+            if i is num_tries - 1:\n+              self.out.permanent_line('Could not remove temporary log file: ' + str(e))\n+            else:\n+              time.sleep(0.1)\n+            continue\n+        break\n+\n+  def log_tasks(self, total_tasks):\n+    self.total_tasks += total_tasks\n+    self.out.transient_line(\"[0/%d] Running tests...\" % self.total_tasks)\n+\n+  def summarize(self, passed_tasks, failed_tasks, interrupted_tasks):\n+    stats = {}\n+    def add_stats(stats, task, idx):\n+      task_key = (task.test_binary, task.test_name)\n+      if not task_key in stats:\n+        # (passed, failed, interrupted) task_key is added as tie breaker to get\n+        # alphabetic sorting on equally-stable tests\n+        stats[task_key] = [0, 0, 0, task_key]\n+      stats[task_key][idx] += 1\n+\n+    for task in passed_tasks:\n+      add_stats(stats, task, 0)\n+    for task in failed_tasks:\n+      add_stats(stats, task, 1)\n+    for task in interrupted_tasks:\n+      add_stats(stats, task, 2)\n+\n+    self.out.permanent_line(\"SUMMARY:\")\n+    for task_key in sorted(stats, key=stats.__getitem__):\n+      (num_passed, num_failed, num_interrupted, _) = stats[task_key]\n+      (test_binary, task_name) = task_key\n+      self.out.permanent_line(\n+          \"  %s %s passed %d / %d times%s.\" %\n+              (test_binary, task_name, num_passed,\n+               num_passed + num_failed + num_interrupted,\n+               \"\" if num_interrupted == 0 else (\" (%d interrupted)\" % num_interrupted)))\n+\n+  def flush(self):\n+    self.out.flush_transient_output()\n+\n+\n+class CollectTestResults(object):\n+  def __init__(self, json_dump_filepath):\n+    self.test_results_lock = threading.Lock()\n+    self.json_dump_file = open(json_dump_filepath, 'w')\n+    self.test_results = {\n+        \"interrupted\": False,\n+        \"path_delimiter\": \".\",\n+        # Third version of the file format. See the link in the flag description\n+        # for details.\n+        \"version\": 3,\n+        \"seconds_since_epoch\": int(time.time()),\n+        \"num_failures_by_type\": {\n+            \"PASS\": 0,\n+            \"FAIL\": 0,\n+        },\n+        \"tests\": {},\n+    }\n+\n+  def log(self, test, runtime_ms, actual_result):\n+    with self.test_results_lock:\n+      self.test_results['num_failures_by_type'][actual_result] += 1\n+      results = self.test_results['tests']\n+      for name in test.split('.'):\n+        results = results.setdefault(name, {})\n+\n+      if results:\n+        results['actual'] += ' ' + actual_result\n+        results['times'].append(runtime_ms)\n+      else:  # This is the first invocation of the test\n+        results['actual'] = actual_result\n+        results['times'] = [runtime_ms]\n+        results['time'] = runtime_ms\n+        results['expected'] = 'PASS'\n+\n+  def dump_to_file_and_close(self):\n+    json.dump(self.test_results, self.json_dump_file)\n+    self.json_dump_file.close()\n+\n+\n+# Record of test runtimes. Has built-in locking.\n+class TestTimes(object):\n+  class LockedFile(object):\n+    def __init__(self, filename, mode):\n+      self._filename = filename\n+      self._mode = mode\n+      self._fo = None\n+\n+    def __enter__(self):\n+      self._fo = open(self._filename, self._mode)\n+\n+      # Regardless of opening mode we always seek to the beginning of file.\n+      # This simplifies code working with LockedFile and also ensures that\n+      # we lock (and unlock below) always the same region in file on win32.\n+      self._fo.seek(0)\n+\n+      try:\n+        if sys.platform == 'win32':\n+          # We are locking here fixed location in file to use it as\n+          # an exclusive lock on entire file.\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_LOCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_EX)\n+      except IOError:\n+        self._fo.close()\n+        raise\n+\n+      return self._fo\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+      # Flush any buffered data to disk. This is needed to prevent race\n+      # condition which happens from the moment of releasing file lock\n+      # till closing the file.\n+      self._fo.flush()\n+\n+      try:\n+        if sys.platform == 'win32':\n+          self._fo.seek(0)\n+          msvcrt.locking(self._fo.fileno(), msvcrt.LK_UNLCK, 1)\n+        else:\n+          fcntl.flock(self._fo.fileno(), fcntl.LOCK_UN)\n+      finally:\n+        self._fo.close()\n+\n+      return exc_value is None\n+\n+  def __init__(self, save_file):\n+    \"Create new object seeded with saved test times from the given file.\"\n+    self.__times = {}  # (test binary, test name) -> runtime in ms\n+\n+    # Protects calls to record_test_time(); other calls are not\n+    # expected to be made concurrently.\n+    self.__lock = threading.Lock()\n+\n+    try:\n+      with TestTimes.LockedFile(save_file, 'rb') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+    except IOError:\n+      # We couldn't obtain the lock.\n+      return\n+\n+    # Discard saved times if the format isn't right.\n+    if type(times) is not dict:\n+      return\n+    for ((test_binary, test_name), runtime) in list(times.items()):\n+      if (type(test_binary) is not str or type(test_name) is not str\n+          or type(runtime) not in {int, long, type(None)}):\n+        return\n+\n+    self.__times = times\n+\n+  def get_test_time(self, binary, testname):\n+    \"\"\"Return the last duration for the given test as an integer number of\n+    milliseconds, or None if the test failed or if there's no record for it.\"\"\"\n+    return self.__times.get((binary, testname), None)\n+\n+  def record_test_time(self, binary, testname, runtime_ms):\n+    \"\"\"Record that the given test ran in the specified number of\n+    milliseconds. If the test failed, runtime_ms should be None.\"\"\"\n+    with self.__lock:\n+      self.__times[(binary, testname)] = runtime_ms\n+\n+  def write_to_file(self, save_file):\n+    \"Write all the times to file.\"\n+    try:\n+      with TestTimes.LockedFile(save_file, 'a+b') as fd:\n+        times = TestTimes.__read_test_times_file(fd)\n+\n+        if times is None:\n+          times = self.__times\n+        else:\n+          times.update(self.__times)\n+\n+        # We erase data from file while still holding a lock to it. This\n+        # way reading old test times and appending new ones are atomic\n+        # for external viewer.\n+        fd.seek(0)\n+        fd.truncate()\n+        with gzip.GzipFile(fileobj=fd, mode='wb') as gzf:\n+          cPickle.dump(times, gzf, PICKLE_HIGHEST_PROTOCOL)\n+    except IOError:\n+      pass  # ignore errors---saving the times isn't that important\n+\n+  @staticmethod\n+  def __read_test_times_file(fd):\n+    try:\n+      with gzip.GzipFile(fileobj=fd, mode='rb') as gzf:\n+        times = cPickle.load(gzf)\n+    except Exception:\n+      # File doesn't exist, isn't readable, is malformed---whatever.\n+      # Just ignore it.\n+      return None\n+    else:\n+      return times\n+\n+\n+def find_tests(test_list, binaries, additional_args, options, times):\n+  test_count = 0\n+  tasks = []\n+  for test_binary in binaries:\n+    command = [test_binary]\n+    command += additional_args\n+\n+    for test_name in test_list:\n+      last_execution_time = times.get_test_time(test_binary, test_name)\n+      if options.failed and last_execution_time is not None:\n+        continue\n+\n+      test_command = command + ['--run_test=' + test_name]\n+      if (test_count - options.shard_index) % options.shard_count == 0:\n+        for execution_number in range(options.repeat):\n+          tasks.append(Task(test_binary, test_name, test_command,\n+                            execution_number + 1, last_execution_time,\n+                            options.output_dir))\n+\n+      test_count += 1\n+\n+  # Sort the tasks to run the slowest tests first, so that faster ones can be\n+  # finished in parallel.\n+  return sorted(tasks, reverse=True)\n+\n+\n+def execute_tasks(tasks, pool_size, task_manager,\n+                  timeout, serialize_test_cases):\n+  class WorkerFn(object):\n+    def __init__(self, tasks, running_groups):\n+      self.tasks = tasks\n+      self.running_groups = running_groups\n+      self.task_lock = threading.Lock()\n+\n+    def __call__(self):\n+      while True:\n+        with self.task_lock:\n+          for task_id in range(len(self.tasks)):\n+            task = self.tasks[task_id]\n+\n+            if self.running_groups is not None:\n+              test_group = task.test_name.split('.')[0]\n+              if test_group in self.running_groups:\n+                # Try to find other non-running test group.\n+                continue\n+              else:\n+                self.running_groups.add(test_group)\n+\n+            del self.tasks[task_id]\n+            break\n+          else:\n+            # Either there is no tasks left or number or remaining test\n+            # cases (groups) is less than number or running threads.\n+            return\n+\n+        task_manager.run_task(task)\n+\n+        if self.running_groups is not None:\n+          with self.task_lock:\n+            self.running_groups.remove(test_group)\n+\n+  def start_daemon(func):\n+    t = threading.Thread(target=func)\n+    t.daemon = True\n+    t.start()\n+    return t\n+\n+  try:\n+    if timeout:\n+      timeout.start()\n+    running_groups = set() if serialize_test_cases else None\n+    worker_fn = WorkerFn(tasks, running_groups)\n+    workers = [start_daemon(worker_fn) for _ in range(pool_size)]\n+    for worker in workers:\n+      worker.join()\n+  finally:\n+    if timeout:\n+      timeout.cancel()\n+\n+\n+def default_options_parser(default_binary, file_test_list):\n+  parser = optparse.OptionParser(\n+      usage = 'usage: %prog [options] binary [binary ...] -- [additional args]')\n+\n+  parser.add_option('-d', '--output_dir', type='string', default=None,\n+                    help='Output directory for test logs. Logs will be '\n+                         'available under gtest-parallel-logs/, so '\n+                         '--output_dir=/tmp will results in all logs being '\n+                         'available under /tmp/gtest-parallel-logs/.')\n+  parser.add_option('-r', '--repeat', type='int', default=1,\n+                    help='Number of times to execute all the tests.')\n+  parser.add_option('--retry_failed', type='int', default=0,\n+                    help='Number of times to repeat failed tests.')\n+  parser.add_option('--failed', action='store_true', default=False,\n+                    help='run only failed and new tests')\n+  parser.add_option('-w', '--workers', type='int',\n+                    default=multiprocessing.cpu_count(),\n+                    help='number of workers to spawn')\n+  parser.add_option('--gtest_color', type='string', default='yes',\n+                    help='color output')\n+  parser.add_option('--gtest_filter', type='string', default='',\n+                    help='test filter')\n+  parser.add_option('--gtest_also_run_disabled_tests', action='store_true',\n+                    default=False, help='run disabled tests too')\n+  parser.add_option('--print_test_times', action='store_true', default=False,\n+                    help='list the run time of each test at the end of execution')\n+  parser.add_option('--shard_count', type='int', default=1,\n+                    help='total number of shards (for sharding test execution '\n+                         'between multiple machines)')\n+  parser.add_option('--shard_index', type='int', default=0,\n+                    help='zero-indexed number identifying this shard (for '\n+                         'sharding test execution between multiple machines)')\n+  parser.add_option('--dump_json_test_results', type='string', default=None,\n+                    help='Saves the results of the tests as a JSON machine-'\n+                         'readable file. The format of the file is specified at '\n+                         'https://www.chromium.org/developers/the-json-test-results-format')\n+  parser.add_option('--timeout', type='int', default=None,\n+                    help='Interrupt all remaining processes after the given '\n+                         'time (in seconds).')\n+  parser.add_option('--serialize_test_cases', action='store_true',",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#discussion_r178238820",
      "id" : 178238820,
      "original_commit_id" : "4292954ebeac41092d3218fdc665bd267530513b",
      "original_position" : 682,
      "path" : "src/test/parallel.py",
      "position" : 682,
      "pull_request_review_id" : 108270259,
      "pull_request_url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/12831",
      "updated_at" : "2018-03-30T06:19:11Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/pulls/comments/178238820",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "author_association" : "CONTRIBUTOR",
      "body" : "Concept ACK - Tested it out and left some initial feedback. Realize it's WIP so just left broad comments. ",
      "created_at" : "2018-03-30T06:21:41Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377455800",
      "id" : 377455800,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-03-30T06:21:41Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377455800",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/14220652?v=4",
         "events_url" : "https://api.github.com/users/conscott/events{/privacy}",
         "followers_url" : "https://api.github.com/users/conscott/followers",
         "following_url" : "https://api.github.com/users/conscott/following{/other_user}",
         "gists_url" : "https://api.github.com/users/conscott/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/conscott",
         "id" : 14220652,
         "login" : "conscott",
         "organizations_url" : "https://api.github.com/users/conscott/orgs",
         "received_events_url" : "https://api.github.com/users/conscott/received_events",
         "repos_url" : "https://api.github.com/users/conscott/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/conscott/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/conscott/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/conscott"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "Thanks for looking at this! I kept the patches to the gtest-parallel script minimal. Feedback not about my patches should be submitted upstream: https://github.com/google/gtest-parallel\r\n\r\nAlso, if someone knows more about autotools, help is very much appreciated to make it run on `make check`.",
      "created_at" : "2018-04-01T22:21:44Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-377821283",
      "id" : 377821283,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-01T22:21:44Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/377821283",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@theuni Mind to give a Concept ACK/NACK or some general comments?",
      "created_at" : "2018-04-05T15:26:48Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-378975587",
      "id" : 378975587,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T15:26:48Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/378975587",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "OWNER",
      "body" : "I have an idea for a simpler approach, where you could tell the test binary there are N processes, and which one out of those it is. It would then partition the tests randomly into N groups, and only run one of them.\n\nIf there's interest, I'll try to implement that soon.",
      "created_at" : "2018-04-05T15:44:23Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-378981463",
      "id" : 378981463,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T15:44:23Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/378981463",
      "user" : {
         "avatar_url" : "https://avatars1.githubusercontent.com/u/548488?v=4",
         "events_url" : "https://api.github.com/users/sipa/events{/privacy}",
         "followers_url" : "https://api.github.com/users/sipa/followers",
         "following_url" : "https://api.github.com/users/sipa/following{/other_user}",
         "gists_url" : "https://api.github.com/users/sipa/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/sipa",
         "id" : 548488,
         "login" : "sipa",
         "organizations_url" : "https://api.github.com/users/sipa/orgs",
         "received_events_url" : "https://api.github.com/users/sipa/received_events",
         "repos_url" : "https://api.github.com/users/sipa/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/sipa/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/sipa/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/sipa"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "@sipa That wouldn't help running the most time-expensive test first or avoiding that two expensive tests end up in the same group?",
      "created_at" : "2018-04-05T15:56:35Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-378985616",
      "id" : 378985616,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T15:56:35Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/378985616",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "OWNER",
      "body" : "@MarcoFalke No, but I think that's an independent problem. If some tests take exorbitantly more time than others, perhaps those tests need to be split up.",
      "created_at" : "2018-04-05T16:15:16Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-378991820",
      "id" : 378991820,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T16:15:16Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/378991820",
      "user" : {
         "avatar_url" : "https://avatars1.githubusercontent.com/u/548488?v=4",
         "events_url" : "https://api.github.com/users/sipa/events{/privacy}",
         "followers_url" : "https://api.github.com/users/sipa/followers",
         "following_url" : "https://api.github.com/users/sipa/following{/other_user}",
         "gists_url" : "https://api.github.com/users/sipa/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/sipa",
         "id" : 548488,
         "login" : "sipa",
         "organizations_url" : "https://api.github.com/users/sipa/orgs",
         "received_events_url" : "https://api.github.com/users/sipa/received_events",
         "repos_url" : "https://api.github.com/users/sipa/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/sipa/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/sipa/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/sipa"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "See #10026 for an (outdated) list of slow unit tests. I haven't checked how practical it is to split them up, but there will always be tests that run slower compared to others.",
      "created_at" : "2018-04-05T16:52:25Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-379003069",
      "id" : 379003069,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T16:52:25Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/379003069",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "The currently longest running test on my machine seems to be \"test_big_witness_transaction\":\r\n\r\n```\r\n$ python3 ./src/test/parallel.py | tail -3\r\n[285/287] streams_tests/streams_serializedata_xor (47 ms)\r\n[286/287] coinselector_tests/knapsack_solver_test (12060 ms)\r\n[287/287] transaction_tests/test_big_witness_transaction (17931 ms)",
      "created_at" : "2018-04-05T17:20:28Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-379011508",
      "id" : 379011508,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T17:20:28Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/379011508",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "* Got rid of test_list.txt\r\n* Run test suites in parallel instead of test cases",
      "created_at" : "2018-04-05T19:58:42Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-379058185",
      "id" : 379058185,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T19:58:42Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/379058185",
      "user" : {
         "avatar_url" : "https://avatars0.githubusercontent.com/u/6399679?v=4",
         "events_url" : "https://api.github.com/users/MarcoFalke/events{/privacy}",
         "followers_url" : "https://api.github.com/users/MarcoFalke/followers",
         "following_url" : "https://api.github.com/users/MarcoFalke/following{/other_user}",
         "gists_url" : "https://api.github.com/users/MarcoFalke/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/MarcoFalke",
         "id" : 6399679,
         "login" : "MarcoFalke",
         "organizations_url" : "https://api.github.com/users/MarcoFalke/orgs",
         "received_events_url" : "https://api.github.com/users/MarcoFalke/received_events",
         "repos_url" : "https://api.github.com/users/MarcoFalke/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/MarcoFalke/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/MarcoFalke/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/MarcoFalke"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "Repeating concept ACK\r\n\r\nAutomatic linting comment: `transform_boost_output_to_test_list` is unused now? :-)",
      "created_at" : "2018-04-05T20:07:23Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-379060530",
      "id" : 379060530,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-05T20:07:33Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/379060530",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   },
   {
      "author_association" : "MEMBER",
      "body" : "NACK. Prefering #12926 :-)",
      "created_at" : "2018-04-09T21:53:09Z",
      "html_url" : "https://github.com/bitcoin/bitcoin/pull/12831#issuecomment-379905694",
      "id" : 379905694,
      "issue_url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/12831",
      "updated_at" : "2018-04-09T21:53:09Z",
      "url" : "https://api.github.com/repos/bitcoin/bitcoin/issues/comments/379905694",
      "user" : {
         "avatar_url" : "https://avatars3.githubusercontent.com/u/7826565?v=4",
         "events_url" : "https://api.github.com/users/practicalswift/events{/privacy}",
         "followers_url" : "https://api.github.com/users/practicalswift/followers",
         "following_url" : "https://api.github.com/users/practicalswift/following{/other_user}",
         "gists_url" : "https://api.github.com/users/practicalswift/gists{/gist_id}",
         "gravatar_id" : "",
         "html_url" : "https://github.com/practicalswift",
         "id" : 7826565,
         "login" : "practicalswift",
         "organizations_url" : "https://api.github.com/users/practicalswift/orgs",
         "received_events_url" : "https://api.github.com/users/practicalswift/received_events",
         "repos_url" : "https://api.github.com/users/practicalswift/repos",
         "site_admin" : false,
         "starred_url" : "https://api.github.com/users/practicalswift/starred{/owner}{/repo}",
         "subscriptions_url" : "https://api.github.com/users/practicalswift/subscriptions",
         "type" : "User",
         "url" : "https://api.github.com/users/practicalswift"
      }
   }
]
